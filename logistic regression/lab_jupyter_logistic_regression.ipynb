{"cells":[{"cell_type":"markdown","id":"a4c4feeb-3c33-4366-bde2-1332e08712b4","metadata":{},"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","id":"8bbd0293-d0d6-4759-aa4d-728e295b19e9","metadata":{},"source":["# **Logistic Regression**\n"]},{"cell_type":"markdown","id":"f2f73dc3-23f2-416c-843d-a483bd0788a0","metadata":{},"source":["Estimated time needed: **30** minutes\n"]},{"cell_type":"markdown","id":"28bf3dc8-2480-444e-ac96-cbefdffae58b","metadata":{},"source":["In this lab, you will learn about and get hands-on practice with the logistic regression model, a popular and effective classification model. Understanding logistic regression and being able to apply it to classification tasks is essential because logistic regression models form the fundamentals of neural networks.\n"]},{"cell_type":"markdown","id":"421dd141-6e91-4852-bd0f-e0aef8d60332","metadata":{},"source":["We will use a real-world dataset that contains detailed nutrition information about food items for people with diabetes. The objective is to classify whether a diabetic patient should choose More Often, Less Often, or In Moderation for a specific food item based on the nutrition information in the dataset.\n"]},{"cell_type":"markdown","id":"9e2dd37d-3d5d-4ae9-9aa6-5bb235862c81","metadata":{},"source":["## Objectives\n"]},{"cell_type":"markdown","id":"9e40045d-c314-43b5-8d57-c195ce3bb6bb","metadata":{},"source":["After completing this lab you will be able to:\n"]},{"cell_type":"markdown","id":"b32b9909-259b-4159-a3a3-d6ead3673420","metadata":{},"source":["*   Preprocess and generate training and testing datasets\n","*   Train and fine-tune logistic regression models\n","*   Interpret trained logistic regression models\n","*   Evaluate trained logistic regression models\n"]},{"cell_type":"markdown","id":"73296749-d994-4f87-ab55-4603ef09718e","metadata":{},"source":["***\n"]},{"cell_type":"markdown","id":"f98eefe5-b524-44f6-a3f0-21eb31a392e2","metadata":{},"source":["## Prepare and setup lab environment\n"]},{"cell_type":"code","execution_count":null,"id":"6c5aa99e-e464-400a-b032-c276f014e925","metadata":{},"outputs":[],"source":["# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n","# !mamba install -qy pandas==1.3.3 numpy==1.21.2 ipywidgets==7.4.2 scipy==7.4.2 tqdm==4.62.3 matplotlib==3.5.0 seaborn==0.9.0\n","# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\""]},{"cell_type":"code","execution_count":null,"id":"4f109cd9-8cd3-4088-9ad9-b08e042c561e","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":1,"id":"6a264f11-4931-40d8-86e7-e7ff82b4377f","metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MinMaxScaler\n","from sklearn.model_selection import train_test_split, learning_curve\n","from sklearn.linear_model import LogisticRegression\n","from sklearn import metrics\n","from sklearn.metrics import classification_report, accuracy_score, confusion_matrix,ConfusionMatrixDisplay, precision_recall_fscore_support, precision_score, recall_score\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline"]},{"cell_type":"code","execution_count":5,"id":"42d0be2a-f7af-4be5-9d96-4fa36fa4d985","metadata":{},"outputs":[],"source":["# also set a random state\n","rs = 123"]},{"cell_type":"markdown","id":"add870f1-5db0-4ffa-87a2-4df1e4405f0f","metadata":{},"source":["### Exploratory Data Analysis(EDA) and Feature Engineering\n","Before we get to the model implementation, it is essential to examine the dataset and carefully select the features that will serve as inputs for the model..\n"]},{"cell_type":"markdown","id":"5c93e072-17db-4f9c-b755-cd011cd2b22f","metadata":{},"source":["### Load and explore the dataset\n"]},{"cell_type":"markdown","id":"03fdc466-436e-41d5-8b69-73e01b4c8aa0","metadata":{},"source":["First, let's load the dataset as a `Pandas` dataframe and conduct some basic EDA tasks on it.\n"]},{"cell_type":"code","execution_count":2,"id":"f34e6143-bfaa-49d8-bf27-a0a8f6c9eed3","metadata":{},"outputs":[],"source":["# Load the dataset\n","dataset_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML241EN-SkillsNetwork/labs/datasets/food_items.csv\"\n","food_df = pd.read_csv(dataset_url)"]},{"cell_type":"markdown","id":"37250b6f-ee6c-4ee8-b1e0-722412eaa0f6","metadata":{},"source":["And, let's quickly check its column types.\n"]},{"cell_type":"code","execution_count":4,"id":"7b673c25-3186-4748-89bc-6dc4b844d1dc","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Calories</th>\n","      <th>Total Fat</th>\n","      <th>Saturated Fat</th>\n","      <th>Monounsaturated Fat</th>\n","      <th>Polyunsaturated Fat</th>\n","      <th>Trans Fat</th>\n","      <th>Cholesterol</th>\n","      <th>Sodium</th>\n","      <th>Total Carbohydrate</th>\n","      <th>Dietary Fiber</th>\n","      <th>Sugars</th>\n","      <th>Sugar Alcohol</th>\n","      <th>Protein</th>\n","      <th>Vitamin A</th>\n","      <th>Vitamin C</th>\n","      <th>Calcium</th>\n","      <th>Iron</th>\n","      <th>class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>12162</th>\n","      <td>20.0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>8</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>'More Often'</td>\n","    </tr>\n","    <tr>\n","      <th>485</th>\n","      <td>97.0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>'In Moderation'</td>\n","    </tr>\n","    <tr>\n","      <th>637</th>\n","      <td>150.0</td>\n","      <td>3</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>20.0</td>\n","      <td>25.0</td>\n","      <td>1.0</td>\n","      <td>18.0</td>\n","      <td>0</td>\n","      <td>5.0</td>\n","      <td>0</td>\n","      <td>50</td>\n","      <td>30</td>\n","      <td>6</td>\n","      <td>'Less Often'</td>\n","    </tr>\n","    <tr>\n","      <th>6782</th>\n","      <td>80.0</td>\n","      <td>3</td>\n","      <td>1.0</td>\n","      <td>0.5</td>\n","      <td>1.5</td>\n","      <td>0.0</td>\n","      <td>50</td>\n","      <td>270.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>12.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>10</td>\n","      <td>2</td>\n","      <td>'Less Often'</td>\n","    </tr>\n","    <tr>\n","      <th>8098</th>\n","      <td>290.0</td>\n","      <td>8</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>20</td>\n","      <td>610.0</td>\n","      <td>40.0</td>\n","      <td>2.0</td>\n","      <td>10.0</td>\n","      <td>0</td>\n","      <td>12.0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>25</td>\n","      <td>10</td>\n","      <td>'Less Often'</td>\n","    </tr>\n","    <tr>\n","      <th>2499</th>\n","      <td>140.0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>20.0</td>\n","      <td>34.0</td>\n","      <td>0.0</td>\n","      <td>16.0</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>'Less Often'</td>\n","    </tr>\n","    <tr>\n","      <th>10968</th>\n","      <td>130.0</td>\n","      <td>3</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>20</td>\n","      <td>135.0</td>\n","      <td>22.0</td>\n","      <td>0.0</td>\n","      <td>14.0</td>\n","      <td>0</td>\n","      <td>4.0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>10</td>\n","      <td>2</td>\n","      <td>'Less Often'</td>\n","    </tr>\n","    <tr>\n","      <th>4525</th>\n","      <td>40.0</td>\n","      <td>3</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>15</td>\n","      <td>30.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>'Less Often'</td>\n","    </tr>\n","    <tr>\n","      <th>5914</th>\n","      <td>120.0</td>\n","      <td>5</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>10</td>\n","      <td>80.0</td>\n","      <td>16.0</td>\n","      <td>2.0</td>\n","      <td>3.0</td>\n","      <td>6</td>\n","      <td>3.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>10</td>\n","      <td>0</td>\n","      <td>'Less Often'</td>\n","    </tr>\n","    <tr>\n","      <th>11758</th>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>'Less Often'</td>\n","    </tr>\n","    <tr>\n","      <th>4903</th>\n","      <td>90.0</td>\n","      <td>8</td>\n","      <td>5.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>25</td>\n","      <td>520.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>5.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>'Less Often'</td>\n","    </tr>\n","    <tr>\n","      <th>5246</th>\n","      <td>180.0</td>\n","      <td>9</td>\n","      <td>3.5</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4</td>\n","      <td>95.0</td>\n","      <td>24.0</td>\n","      <td>1.0</td>\n","      <td>13.0</td>\n","      <td>0</td>\n","      <td>2.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>'Less Often'</td>\n","    </tr>\n","    <tr>\n","      <th>7425</th>\n","      <td>400.0</td>\n","      <td>16</td>\n","      <td>5.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>20</td>\n","      <td>780.0</td>\n","      <td>51.0</td>\n","      <td>7.0</td>\n","      <td>8.0</td>\n","      <td>0</td>\n","      <td>13.0</td>\n","      <td>10</td>\n","      <td>25</td>\n","      <td>20</td>\n","      <td>15</td>\n","      <td>'Less Often'</td>\n","    </tr>\n","    <tr>\n","      <th>9802</th>\n","      <td>5.0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>180.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>'More Often'</td>\n","    </tr>\n","    <tr>\n","      <th>8956</th>\n","      <td>200.0</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>41.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>7.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>10</td>\n","      <td>'In Moderation'</td>\n","    </tr>\n","    <tr>\n","      <th>1810</th>\n","      <td>5.0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>'More Often'</td>\n","    </tr>\n","    <tr>\n","      <th>11017</th>\n","      <td>150.0</td>\n","      <td>9</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>5.0</td>\n","      <td>14.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>2.0</td>\n","      <td>0</td>\n","      <td>10</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>'Less Often'</td>\n","    </tr>\n","    <tr>\n","      <th>12573</th>\n","      <td>80.0</td>\n","      <td>2</td>\n","      <td>0.0</td>\n","      <td>0.5</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>95.0</td>\n","      <td>10.0</td>\n","      <td>1.0</td>\n","      <td>7.0</td>\n","      <td>0</td>\n","      <td>6.0</td>\n","      <td>10</td>\n","      <td>0</td>\n","      <td>30</td>\n","      <td>6</td>\n","      <td>'In Moderation'</td>\n","    </tr>\n","    <tr>\n","      <th>5595</th>\n","      <td>70.0</td>\n","      <td>1</td>\n","      <td>0.5</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>5</td>\n","      <td>30.0</td>\n","      <td>14.0</td>\n","      <td>0.0</td>\n","      <td>12.0</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>'In Moderation'</td>\n","    </tr>\n","    <tr>\n","      <th>12838</th>\n","      <td>100.0</td>\n","      <td>3</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>300.0</td>\n","      <td>6.0</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>0</td>\n","      <td>13.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>6</td>\n","      <td>'In Moderation'</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       Calories  Total Fat  Saturated Fat  Monounsaturated Fat  \\\n","12162      20.0          0            0.0                  0.0   \n","485        97.0          0            0.0                  0.0   \n","637       150.0          3            0.0                  0.0   \n","6782       80.0          3            1.0                  0.5   \n","8098      290.0          8            4.0                  0.0   \n","2499      140.0          0            0.0                  0.0   \n","10968     130.0          3            2.0                  0.0   \n","4525       40.0          3            2.0                  0.0   \n","5914      120.0          5            2.0                  0.0   \n","11758       0.0          0            0.0                  0.0   \n","4903       90.0          8            5.0                  0.0   \n","5246      180.0          9            3.5                  0.0   \n","7425      400.0         16            5.0                  0.0   \n","9802        5.0          0            0.0                  0.0   \n","8956      200.0          1            0.0                  0.0   \n","1810        5.0          0            0.0                  0.0   \n","11017     150.0          9            2.0                  0.0   \n","12573      80.0          2            0.0                  0.5   \n","5595       70.0          1            0.5                  0.0   \n","12838     100.0          3            0.0                  0.0   \n","\n","       Polyunsaturated Fat  Trans Fat  Cholesterol  Sodium  \\\n","12162                  0.0        0.0            0     0.0   \n","485                    0.0        0.0            0     0.0   \n","637                    2.0        0.0            0    20.0   \n","6782                   1.5        0.0           50   270.0   \n","8098                   0.0        0.0           20   610.0   \n","2499                   0.0        0.0            0    20.0   \n","10968                  0.0        0.0           20   135.0   \n","4525                   0.0        0.0           15    30.0   \n","5914                   0.0        0.0           10    80.0   \n","11758                  0.0        0.0            0     0.0   \n","4903                   0.0        0.0           25   520.0   \n","5246                   0.0        0.0            4    95.0   \n","7425                   0.0        0.0           20   780.0   \n","9802                   0.0        0.0            0   180.0   \n","8956                   0.0        0.0            0     0.0   \n","1810                   0.0        0.0            0     0.0   \n","11017                  0.0        0.0            0     5.0   \n","12573                  1.0        0.0            0    95.0   \n","5595                   0.0        0.0            5    30.0   \n","12838                  0.0        0.0            0   300.0   \n","\n","       Total Carbohydrate  Dietary Fiber  Sugars  Sugar Alcohol  Protein  \\\n","12162                 0.0            0.0     1.0              0      1.0   \n","485                   0.0            0.0     0.0              0      0.0   \n","637                  25.0            1.0    18.0              0      5.0   \n","6782                  0.0            0.0     0.0              0     12.0   \n","8098                 40.0            2.0    10.0              0     12.0   \n","2499                 34.0            0.0    16.0              0      1.0   \n","10968                22.0            0.0    14.0              0      4.0   \n","4525                  1.0            0.0     1.0              0      1.0   \n","5914                 16.0            2.0     3.0              6      3.0   \n","11758                 0.0            0.0     0.0              0      0.0   \n","4903                  1.0            0.0     0.0              0      5.0   \n","5246                 24.0            1.0    13.0              0      2.0   \n","7425                 51.0            7.0     8.0              0     13.0   \n","9802                  0.0            0.0     0.0              0      0.0   \n","8956                 41.0            2.0     1.0              0      7.0   \n","1810                  0.0            0.0     0.0              0      0.0   \n","11017                14.0            1.0     0.0              0      2.0   \n","12573                10.0            1.0     7.0              0      6.0   \n","5595                 14.0            0.0    12.0              0      1.0   \n","12838                 6.0            3.0     1.0              0     13.0   \n","\n","       Vitamin A  Vitamin C  Calcium  Iron            class  \n","12162          8          4        2     2     'More Often'  \n","485            0          0        0     0  'In Moderation'  \n","637            0         50       30     6     'Less Often'  \n","6782           0          0       10     2     'Less Often'  \n","8098           4          0       25    10     'Less Often'  \n","2499           0          0        0     0     'Less Often'  \n","10968          2          2       10     2     'Less Often'  \n","4525           2          0        4     0     'Less Often'  \n","5914           0          0       10     0     'Less Often'  \n","11758          0          0        0     0     'Less Often'  \n","4903           0          0        0     0     'Less Often'  \n","5246           0          0        0     2     'Less Often'  \n","7425          10         25       20    15     'Less Often'  \n","9802           0          0        0     0     'More Often'  \n","8956           0          0        0    10  'In Moderation'  \n","1810           0          0        0     0     'More Often'  \n","11017          0         10        0     2     'Less Often'  \n","12573         10          0       30     6  'In Moderation'  \n","5595           0          0        4     0  'In Moderation'  \n","12838          0          0        2     6  'In Moderation'  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["food_df.sample(20)"]},{"cell_type":"markdown","id":"18f376dc-1e75-40be-9a25-d924185806fb","metadata":{},"source":["Print the first ten food items:\n"]},{"cell_type":"code","execution_count":null,"id":"7f899b29-f894-42f8-9014-0e3b9d03f2bd","metadata":{},"outputs":[],"source":["\n","food_df.head(10)"]},{"cell_type":"markdown","id":"42636224-1480-462a-b13b-6c8e56c49f4a","metadata":{},"source":["Get the row entries with col 0 to -1 (16).\n"]},{"cell_type":"code","execution_count":6,"id":"51bda2c3-235b-42e9-94b6-7bf83b12a5a3","metadata":{},"outputs":[{"data":{"text/plain":["['Calories',\n"," 'Total Fat',\n"," 'Saturated Fat',\n"," 'Monounsaturated Fat',\n"," 'Polyunsaturated Fat',\n"," 'Trans Fat',\n"," 'Cholesterol',\n"," 'Sodium',\n"," 'Total Carbohydrate',\n"," 'Dietary Fiber',\n"," 'Sugars',\n"," 'Sugar Alcohol',\n"," 'Protein',\n"," 'Vitamin A',\n"," 'Vitamin C',\n"," 'Calcium',\n"," 'Iron']"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["\n","feature_cols = list(food_df.iloc[:, :-1].columns)\n","feature_cols"]},{"cell_type":"markdown","id":"92fd1c08-c1f1-4b01-8b8c-40e3379186c6","metadata":{},"source":["Obtain descriptive statistics:\n"]},{"cell_type":"code","execution_count":7,"id":"daae9545-4d51-4020-b3ac-3ce16104eb91","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Calories</th>\n","      <th>Total Fat</th>\n","      <th>Saturated Fat</th>\n","      <th>Monounsaturated Fat</th>\n","      <th>Polyunsaturated Fat</th>\n","      <th>Trans Fat</th>\n","      <th>Cholesterol</th>\n","      <th>Sodium</th>\n","      <th>Total Carbohydrate</th>\n","      <th>Dietary Fiber</th>\n","      <th>Sugars</th>\n","      <th>Sugar Alcohol</th>\n","      <th>Protein</th>\n","      <th>Vitamin A</th>\n","      <th>Vitamin C</th>\n","      <th>Calcium</th>\n","      <th>Iron</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>13260.000000</td>\n","      <td>13260.000000</td>\n","      <td>13260.000000</td>\n","      <td>13260.000000</td>\n","      <td>13260.000000</td>\n","      <td>13260.000000</td>\n","      <td>13260.000000</td>\n","      <td>13260.000000</td>\n","      <td>13260.000000</td>\n","      <td>13260.000000</td>\n","      <td>13260.000000</td>\n","      <td>13260.000000</td>\n","      <td>13260.000000</td>\n","      <td>13260.000000</td>\n","      <td>13260.000000</td>\n","      <td>13260.000000</td>\n","      <td>13260.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>133.861086</td>\n","      <td>4.475264</td>\n","      <td>1.450617</td>\n","      <td>0.338069</td>\n","      <td>0.254660</td>\n","      <td>0.047459</td>\n","      <td>8.857692</td>\n","      <td>241.867142</td>\n","      <td>18.232020</td>\n","      <td>1.602971</td>\n","      <td>6.645234</td>\n","      <td>0.117949</td>\n","      <td>4.661333</td>\n","      <td>6.287632</td>\n","      <td>6.741855</td>\n","      <td>5.175264</td>\n","      <td>5.235671</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>94.227650</td>\n","      <td>5.386340</td>\n","      <td>2.410318</td>\n","      <td>1.345852</td>\n","      <td>2.230586</td>\n","      <td>0.321402</td>\n","      <td>20.976530</td>\n","      <td>272.284363</td>\n","      <td>14.786316</td>\n","      <td>3.363879</td>\n","      <td>8.328465</td>\n","      <td>1.121529</td>\n","      <td>5.611143</td>\n","      <td>18.374191</td>\n","      <td>23.785100</td>\n","      <td>8.779637</td>\n","      <td>9.119459</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>70.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>40.000000</td>\n","      <td>5.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>120.000000</td>\n","      <td>3.000000</td>\n","      <td>0.500000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>135.000000</td>\n","      <td>17.000000</td>\n","      <td>1.000000</td>\n","      <td>3.000000</td>\n","      <td>0.000000</td>\n","      <td>3.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>2.000000</td>\n","      <td>2.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>180.000000</td>\n","      <td>7.000000</td>\n","      <td>2.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>10.000000</td>\n","      <td>370.000000</td>\n","      <td>27.000000</td>\n","      <td>2.000000</td>\n","      <td>11.000000</td>\n","      <td>0.000000</td>\n","      <td>7.000000</td>\n","      <td>6.000000</td>\n","      <td>2.000000</td>\n","      <td>6.000000</td>\n","      <td>8.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>2210.000000</td>\n","      <td>43.000000</td>\n","      <td>22.000000</td>\n","      <td>40.000000</td>\n","      <td>235.000000</td>\n","      <td>11.000000</td>\n","      <td>450.000000</td>\n","      <td>2431.000000</td>\n","      <td>270.000000</td>\n","      <td>305.000000</td>\n","      <td>115.000000</td>\n","      <td>31.000000</td>\n","      <td>70.000000</td>\n","      <td>622.000000</td>\n","      <td>1000.000000</td>\n","      <td>110.000000</td>\n","      <td>170.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           Calories     Total Fat  Saturated Fat  Monounsaturated Fat  \\\n","count  13260.000000  13260.000000   13260.000000         13260.000000   \n","mean     133.861086      4.475264       1.450617             0.338069   \n","std       94.227650      5.386340       2.410318             1.345852   \n","min        0.000000      0.000000       0.000000             0.000000   \n","25%       70.000000      0.000000       0.000000             0.000000   \n","50%      120.000000      3.000000       0.500000             0.000000   \n","75%      180.000000      7.000000       2.000000             0.000000   \n","max     2210.000000     43.000000      22.000000            40.000000   \n","\n","       Polyunsaturated Fat     Trans Fat   Cholesterol        Sodium  \\\n","count         13260.000000  13260.000000  13260.000000  13260.000000   \n","mean              0.254660      0.047459      8.857692    241.867142   \n","std               2.230586      0.321402     20.976530    272.284363   \n","min               0.000000      0.000000      0.000000      0.000000   \n","25%               0.000000      0.000000      0.000000     40.000000   \n","50%               0.000000      0.000000      0.000000    135.000000   \n","75%               0.000000      0.000000     10.000000    370.000000   \n","max             235.000000     11.000000    450.000000   2431.000000   \n","\n","       Total Carbohydrate  Dietary Fiber        Sugars  Sugar Alcohol  \\\n","count        13260.000000   13260.000000  13260.000000   13260.000000   \n","mean            18.232020       1.602971      6.645234       0.117949   \n","std             14.786316       3.363879      8.328465       1.121529   \n","min              0.000000       0.000000      0.000000       0.000000   \n","25%              5.000000       0.000000      0.000000       0.000000   \n","50%             17.000000       1.000000      3.000000       0.000000   \n","75%             27.000000       2.000000     11.000000       0.000000   \n","max            270.000000     305.000000    115.000000      31.000000   \n","\n","            Protein     Vitamin A     Vitamin C       Calcium          Iron  \n","count  13260.000000  13260.000000  13260.000000  13260.000000  13260.000000  \n","mean       4.661333      6.287632      6.741855      5.175264      5.235671  \n","std        5.611143     18.374191     23.785100      8.779637      9.119459  \n","min        0.000000      0.000000      0.000000      0.000000      0.000000  \n","25%        1.000000      0.000000      0.000000      0.000000      0.000000  \n","50%        3.000000      0.000000      0.000000      2.000000      2.000000  \n","75%        7.000000      6.000000      2.000000      6.000000      8.000000  \n","max       70.000000    622.000000   1000.000000    110.000000    170.000000  "]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["food_df.iloc[:, :-1].describe()"]},{"cell_type":"markdown","id":"33e9ec1a-b75a-4f72-9bb7-4a7cb244b87a","metadata":{},"source":["As we can see from the above output, this dataset contains 17 nutrient categories about each food item. These categories include Calories, Total Fat, Protein, Sugar, etc., and are listed as numeric variables. As such, we only need to scale them for training our logistic regression model so that we can compare our feature coefficients directly. This will be done under the feature engineering section.\n"]},{"cell_type":"markdown","id":"a0785d83-6ff3-4f2b-a3d9-c6eb5543990b","metadata":{},"source":["Next, let's check the target variable in the `class` column to see the label values and their distribution.\n"]},{"cell_type":"code","execution_count":9,"id":"dd1b89da-0d4f-4e7e-b4e2-e46624807cd6","metadata":{},"outputs":[{"data":{"text/plain":["class          \n","'In Moderation'    6649\n","'Less Often'       5621\n","'More Often'        990\n","Name: count, dtype: int64"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# # Get the row entries with the last col 'class'\n","food_df.iloc[:, -1:].value_counts()"]},{"cell_type":"code","execution_count":10,"id":"13e4d4fd-9817-43d2-a140-7966a66052aa","metadata":{},"outputs":[{"data":{"text/plain":["<Axes: xlabel='class'>"]},"execution_count":10,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["food_df.iloc[:, -1:].value_counts().plot.bar(color=['yellow', 'red', 'green'])"]},{"cell_type":"markdown","id":"33451c19-ac01-4459-acb4-f733e7f6cad6","metadata":{},"source":["As we can see from the bar chart above, this dataset has three classes: `In Moderation`, `Less Often`, and `More Often`. The three labels are imbalanced. For diabetic patients, most food items are in the In Moderation and Less Often categories. This makes diabetes diet management very hard, so we could build a machine learning model to help patients choose their food.\n"]},{"cell_type":"markdown","id":"c9f5194e-d0eb-487f-9c64-6110d5dab9d7","metadata":{},"source":["We have three labels meaning our logistic regression model will be multinomial with three classes.\n","\n","A multinomial logistic regression is a generalized logistic regression model which generates a probability distribution over all classes, based on the logits or exponentiated log-odds calculated for each class (usually more than two).\n"]},{"cell_type":"markdown","id":"1160a184-a1d4-4722-82c7-c5ad81a1c1f5","metadata":{},"source":["Also note that a multinomial logistic regression model is different from the `one-vs-rest` binary logistic regression. For `one-vs-rest` schema, you need to train an independent classifier for each class. For example, you need a `More Often` classifier to differentiate a food item between `More Often` and `Not More Often` (or, `In Moderation` and `Less Often`).\n"]},{"cell_type":"markdown","id":"2e48158d-228a-4088-a95a-e649cfcaec74","metadata":{},"source":["### Feature Engineering\n"]},{"cell_type":"markdown","id":"844c25d3-a560-4467-b848-79c01f2a9c70","metadata":{},"source":["Now you should have some basic understanding about the food dataset. Next, let's process the raw dataset and construct input data `X` and label/output `y` for logistic regression model training.\n"]},{"cell_type":"code","execution_count":11,"id":"062e3232-9180-4d17-99b0-323a47547b6e","metadata":{},"outputs":[],"source":["X_raw = food_df.iloc[:, :-1]\n","y_raw = food_df.iloc[:, -1:]"]},{"cell_type":"markdown","id":"6bffebed-e86e-4e61-9533-5e1b9384debd","metadata":{},"source":["Fortunately, all feature columns are numeric so we just need to scale them. Here we use the `MinMaxScaler` provided by `sklearn` for scaling.\n"]},{"cell_type":"code","execution_count":12,"id":"fc84c8f4-f6dd-4e4d-88e7-938fbd953936","metadata":{},"outputs":[],"source":["# Create a MinMaxScaler object\n","scaler = MinMaxScaler()"]},{"cell_type":"code","execution_count":13,"id":"c952999c-2059-4440-b6ae-7379180e813f","metadata":{},"outputs":[],"source":["# Scaling the raw input features\n","X = scaler.fit_transform(X_raw)"]},{"cell_type":"markdown","id":"4bbd7336-5f1e-405f-bc28-15d994cf648d","metadata":{},"source":["Let's check the scaled feature value range:\n"]},{"cell_type":"code","execution_count":14,"id":"e849462b-6486-4ce2-983f-d516ce38ad83","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The range of feature inputs are within 0.0 to 1.0\n"]}],"source":["print(f\"The range of feature inputs are within {X.min()} to {X.max()}\")"]},{"cell_type":"markdown","id":"e95cb718-97f4-4aa9-9589-f4b722d67872","metadata":{},"source":["For the target variable `y`, let's use the `LabelEncoder` provided by `sklearn` to encode its three class values.\n"]},{"cell_type":"code","execution_count":15,"id":"c9b62021-524d-4d81-bb16-9e5036cbe5c8","metadata":{},"outputs":[],"source":["# Create a LabelEncoder object\n","label_encoder = LabelEncoder()"]},{"cell_type":"code","execution_count":16,"id":"c166dd35-5b6f-4115-a807-ff6d04f3fce1","metadata":{},"outputs":[],"source":["# Encode the target variable\n","y = label_encoder.fit_transform(y_raw.values.ravel())\n","# Note that ravel() function flattens the vector."]},{"cell_type":"markdown","id":"1b8e14df-fb88-4560-8046-e62a63c133a9","metadata":{},"source":["The encoded target variable will only contain values `0=In Moderation`, `1=Less Often`, `2=More Often`.\n"]},{"cell_type":"code","execution_count":17,"id":"dcd296a4-75fe-41e7-815a-03efd9b414ea","metadata":{},"outputs":[{"data":{"text/plain":["(array([0, 1, 2]), array([6649, 5621,  990]))"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["np.unique(y, return_counts=True)"]},{"cell_type":"markdown","id":"956947aa-a138-439a-a7f0-723517eae62e","metadata":{},"source":["## Train logistic regression models\n"]},{"cell_type":"markdown","id":"4d996c57-f404-4be6-8bfa-a97f0736d046","metadata":{},"source":["First, let's split the dataset into a training and a testing dataset. Training dataset will be used to train and (maybe) tune models, and testing dataset will be used to evaluate the models. Note that you may also split the training dataset into train and validation sets where the validation dataset is only used to tune the model and to set the model parameters.\n"]},{"cell_type":"code","execution_count":18,"id":"7ced04d1-ff07-493f-b425-00f6fa283847","metadata":{},"outputs":[],"source":["# First, let's split the training and testing dataset\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state = rs)"]},{"cell_type":"markdown","id":"c45d635b-1615-4ca9-9f08-a36d5a692594","metadata":{},"source":["Let's look at the shapes of the split datasets:\n"]},{"cell_type":"code","execution_count":null,"id":"17125def-1cf2-423e-8a43-87da62a0aa58","metadata":{},"outputs":[],"source":["print(f\"Training dataset shape, X_train: {X_train.shape}, y_train: {y_train.shape}\")"]},{"cell_type":"code","execution_count":null,"id":"c46be02f-6d14-48c7-a632-699468a22d02","metadata":{},"outputs":[],"source":["print(f\"Testing dataset shape, X_test: {X_test.shape}, y_test: {y_test.shape}\")"]},{"cell_type":"markdown","id":"ddec660e-3917-48c6-a48b-c06422f1f4bf","metadata":{},"source":["OK, now we have the training and testing datasets ready, let's start the model training task.\n"]},{"cell_type":"markdown","id":"8a64c18b-ec34-4315-9294-7e652df00a40","metadata":{},"source":["We first define a `sklearn.linear_model.LogisticRegression` model with the following arguments, you can check the comment for each argument for what it means.\n"]},{"cell_type":"code","execution_count":19,"id":"b18e283a-f43a-4e4c-813d-32828cbca358","metadata":{},"outputs":[],"source":["# L2 penalty to shrink coefficients without removing any features from the model\n","penalty= 'l2'\n","# Our classification problem is multinomial\n","multi_class = 'multinomial'\n","# Use lbfgs for L2 penalty and multinomial classes\n","solver = 'lbfgs'\n","# Max iteration = 1000\n","max_iter = 1000"]},{"cell_type":"code","execution_count":null,"id":"064a29f6-28fc-401e-9c02-e492cd989c1f","metadata":{},"outputs":[],"source":["# Define a logistic regression model with above arguments\n","l2_model = LogisticRegression(random_state=rs, penalty=penalty, multi_class=multi_class, solver=solver, max_iter=max_iter)"]},{"cell_type":"markdown","id":"1505c1f2-6235-4499-a04c-20d3e8b9d9b5","metadata":{},"source":["Let's train the model with training input data `X_train` and labels `y_train`:\n"]},{"cell_type":"code","execution_count":null,"id":"72422ed9-f3f2-4736-a6ad-0e0bdc010821","metadata":{},"outputs":[],"source":["l2_model.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"id":"197a0dac-7835-46be-8e12-8f6242c61679","metadata":{},"outputs":[],"source":["l2_preds = l2_model.predict(X_test)"]},{"cell_type":"markdown","id":"3285be00-384c-413f-992d-fade1b036acd","metadata":{},"source":["Because we may need to evaluate the model multiple times with different model hyper parameters, here we define an utility method to take the ground truths `y_test` and the predictions `preds`, and return a Python `dict` with `accuracy`, `recall`, `precision`, and `f1score`.\n"]},{"cell_type":"code","execution_count":20,"id":"b2d85844-78c2-4fe5-a31a-1251fa2b24f6","metadata":{},"outputs":[],"source":["def evaluate_metrics(yt, yp):\n","    results_pos = {}\n","    results_pos['accuracy'] = accuracy_score(yt, yp)\n","    precision, recall, f_beta, _ = precision_recall_fscore_support(yt, yp)\n","    results_pos['recall'] = recall\n","    results_pos['precision'] = precision\n","    results_pos['f1score'] = f_beta\n","    return results_pos"]},{"cell_type":"code","execution_count":21,"id":"1f937ec5-5457-458c-b7b8-c74d6f4fa77a","metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'l2_preds' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m evaluate_metrics(y_test, l2_preds)\n","\u001b[0;31mNameError\u001b[0m: name 'l2_preds' is not defined"]}],"source":["evaluate_metrics(y_test, l2_preds)"]},{"cell_type":"markdown","id":"814dedca-d642-4dbd-98a4-175bb1cc7c37","metadata":{},"source":["As we can see from  the above evaluation results, the logistic regression model has relatively good performance on this multinomial classification task. The overall accuracy is around `0.77` and the f1score is around `0.8`. Note that for `recall`, `precision`, and `f1score`, we output the values for each class to see how the model performs on an individual class. And, we can see from the results, the recall for `class=2` (More often) is not very good. This is actually a common problem called imbalanced classification challenge. We will introduce solution to this problem later in this course.\n"]},{"cell_type":"markdown","id":"e93e529a-7937-4e4b-903d-26f1fd9d8d2e","metadata":{},"source":["Next, let's try defining another logistic regression model with l1 penality this time, to see if our classification performance would be improved.\n"]},{"cell_type":"code","execution_count":22,"id":"2549a767-05a7-4bb7-bfba-f23095d91e87","metadata":{},"outputs":[],"source":["# L1 penalty to shrink coefficients without removing any features from the model\n","penalty= 'l1'\n","# Our classification problem is multinomial\n","multi_class = 'multinomial'\n","# Use saga for L1 penalty and multinomial classes\n","solver = 'saga'\n","# Max iteration = 1000\n","max_iter = 1000"]},{"cell_type":"markdown","id":"079724ef-8f25-47cc-b232-2685033716a5","metadata":{},"source":["Then we define another logistic regression model with above arguments using l1 penality and related solver.\n"]},{"cell_type":"code","execution_count":30,"id":"00750e56-c1ef-4603-8af1-d3916ae9f5f9","metadata":{},"outputs":[],"source":["# Define a logistic regression model with above arguments\n","l1_model = LogisticRegression(random_state=rs, penalty=penalty, multi_class=multi_class, solver=solver, max_iter = 1000)"]},{"cell_type":"markdown","id":"24ce1e97-4732-4027-909a-c1f29d68ddf6","metadata":{},"source":["We can start to train the new `l1_model` with the new taining dataset.\n"]},{"cell_type":"code","execution_count":31,"id":"4e435349-f101-4227-a0f1-7ea3a46c6bf1","metadata":{},"outputs":[{"data":{"text/html":["<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000, multi_class=&#x27;multinomial&#x27;, penalty=&#x27;l1&#x27;,\n","                   random_state=123, solver=&#x27;saga&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, multi_class=&#x27;multinomial&#x27;, penalty=&#x27;l1&#x27;,\n","                   random_state=123, solver=&#x27;saga&#x27;)</pre></div></div></div></div></div>"],"text/plain":["LogisticRegression(max_iter=1000, multi_class='multinomial', penalty='l1',\n","                   random_state=123, solver='saga')"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["l1_model.fit(X_train, y_train)"]},{"cell_type":"markdown","id":"f28ab6df-50ef-4992-8cb3-5aa25c3edf15","metadata":{},"source":["And, make predictions using the input in the test dataset.\n"]},{"cell_type":"code","execution_count":42,"id":"4bb69f1b-aa73-4aac-8c3d-f453fa1b06d9","metadata":{},"outputs":[{"data":{"text/plain":["array([1, 1, 1, ..., 0, 0, 1])"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["l1_preds = l1_model.predict(X_test)\n","l1_preds"]},{"cell_type":"markdown","id":"aec6a2eb-c5a3-4003-9322-17f350d7624f","metadata":{},"source":["We can also check the class probability distribution using the `predict_proba` function. For example, we want to see the probabilities of belonging to each class for the first instance in the test dataset:\n"]},{"cell_type":"code","execution_count":33,"id":"bebf4379-ca24-4a4a-a761-f9af56d28850","metadata":{},"outputs":[{"data":{"text/plain":["array([3.55065558e-02, 9.64491964e-01, 1.47991419e-06])"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["odd_ratios = l1_model.predict_proba(X_test[:1, :])[0]\n","odd_ratios"]},{"cell_type":"markdown","id":"56a50a33-cac1-4463-9e24-ac07140f7a76","metadata":{},"source":["We can see that  Class 1 has the largest probability 0.96. As such, the model prediction for this instance will be class `1` and this is the same as the `predict` method.\n"]},{"cell_type":"code","execution_count":41,"id":"bbefda8c-2648-435d-92d4-8fafa51c2361","metadata":{},"outputs":[{"data":{"text/plain":["array([1])"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["l1_model.predict(X_test[:1, :])"]},{"cell_type":"markdown","id":"604a89da-8985-4a65-8b79-c6a4e8367149","metadata":{},"source":["Given the true labels (`y_test`) and predictions, we can evaluate the model performance by calling the utility `evaluate_metrics`  method.\n"]},{"cell_type":"code","execution_count":35,"id":"76b55955-e072-4fff-96d2-c68d805591d3","metadata":{},"outputs":[{"data":{"text/plain":["{'accuracy': 0.8092006033182504,\n"," 'recall': array([0.85488722, 0.74377224, 0.87373737]),\n"," 'precision': array([0.78848821, 0.83516484, 0.8277512 ]),\n"," 'f1score': array([0.82034632, 0.78682353, 0.85012285])}"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["evaluate_metrics(y_test, l1_preds)"]},{"cell_type":"markdown","id":"9876436b-32af-4cb6-84e0-3c65fa2bdedc","metadata":{},"source":["Now, we can see this logistic regression with l1 penalty has much better performance than l2. One possible reason is that l1 penalty may remove some correlated feature variables by shrinking their coefficents to zero. As such, the model is much simplified to avoid overfitting on the training data and better aligned with the logistic regression assumption that all features should be independent.\n"]},{"cell_type":"markdown","id":"022467ca-52d5-485a-8c27-dfb634e85abe","metadata":{},"source":["### Confusion Matrix\n"]},{"cell_type":"markdown","id":"82961fcb-1ef3-49fb-8d15-cb4685182185","metadata":{},"source":["We can also plot the confusion matrix based on the true labels and predictions using the `confusion_matrix` method provided by `sklearn`,\n"]},{"cell_type":"code","execution_count":null,"id":"cf40f5c6-6fe6-4604-ba78-58aa6ca86298","metadata":{},"outputs":[],"source":["cf = confusion_matrix(y_test, l1_preds, normalize='true')"]},{"cell_type":"markdown","id":"58266b65-4d05-4842-aa85-cb1e944a4dd8","metadata":{},"source":["and easily visualize it using a heatmap method provided by `seaborn`.\n"]},{"cell_type":"code","execution_count":null,"id":"79cf9c42-68a1-4a04-bce7-d49db22ca6c5","metadata":{},"outputs":[],"source":["sns.set_context('talk')\n","disp = ConfusionMatrixDisplay(confusion_matrix=cf,display_labels=l1_model.classes_)\n","disp.plot()\n","plt.show()"]},{"cell_type":"markdown","id":"ec22a680-1a10-4e18-8058-5e4d55848e6a","metadata":{},"source":["### Interpret logistic regression models\n"]},{"cell_type":"markdown","id":"237f5a1f-e532-4865-b977-a88c7612d172","metadata":{},"source":["One way to interpret logistic regression models is by analyzing feature coefficients. Although it may not be as effective as the regular linear regression models because the logistic regression model has a sigmoid function, we can still get a sense for the importance or impact of each feature.  \n"]},{"cell_type":"markdown","id":"54505801-404c-4645-9799-9ae732a8d9a9","metadata":{},"source":["We can check the coefficients for logistic regression model using its `coef_` attribute:\n"]},{"cell_type":"code","execution_count":null,"id":"a6f88e25-78e5-4859-a40b-00a67374d46a","metadata":{},"outputs":[],"source":["l1_model.coef_"]},{"cell_type":"markdown","id":"9e2f3493-a15e-4612-a998-a057d173b49a","metadata":{},"source":["The `coef_` is a coefficients list with three elements, one element is the actual coefficent for class 0, 1, 2. To better analyze the coefficients, let's use three utility methods to sort and visualize them.\n"]},{"cell_type":"code","execution_count":null,"id":"0a771acc-7480-42bd-9810-ffbd790f3cec","metadata":{},"outputs":[],"source":["# Extract and sort feature coefficients\n","def get_feature_coefs(regression_model, label_index, columns):\n","    coef_dict = {}\n","    for coef, feat in zip(regression_model.coef_[label_index, :], columns):\n","        if abs(coef) >= 0.01:\n","            coef_dict[feat] = coef\n","    # Sort coefficients\n","    coef_dict = {k: v for k, v in sorted(coef_dict.items(), key=lambda item: item[1])}\n","    return coef_dict\n","\n","# Generate bar colors based on if value is negative or positive\n","def get_bar_colors(values):\n","    color_vals = []\n","    for val in values:\n","        if val <= 0:\n","            color_vals.append('r')\n","        else:\n","            color_vals.append('g')\n","    return color_vals\n","\n","# Visualize coefficients\n","def visualize_coefs(coef_dict):\n","    features = list(coef_dict.keys())\n","    values = list(coef_dict.values())\n","    y_pos = np.arange(len(features))\n","    color_vals = get_bar_colors(values)\n","    plt.rcdefaults()\n","    fig, ax = plt.subplots()\n","    ax.barh(y_pos, values, align='center', color=color_vals)\n","    ax.set_yticks(y_pos)\n","    ax.set_yticklabels(features)\n","    # labels read top-to-bottom\n","    ax.invert_yaxis()  \n","    ax.set_xlabel('Feature Coefficients')\n","    ax.set_title('')\n","    plt.show()"]},{"cell_type":"markdown","id":"0a8fce0b-2e8f-4e10-a439-830ece228c35","metadata":{},"source":["Then, let's visualize the sorted coefficient for class 1, the `Less Often` class: \n"]},{"cell_type":"code","execution_count":null,"id":"df5f9dec-034f-47ce-9e09-b88a1c51f6d0","metadata":{},"outputs":[],"source":["# Get the coefficents for Class 1, Less Often\n","coef_dict = get_feature_coefs(l1_model, 1, feature_cols)"]},{"cell_type":"code","execution_count":null,"id":"addca7bf-e483-4f54-9c3a-a5bf8a3d8346","metadata":{},"outputs":[],"source":["visualize_coefs(coef_dict)"]},{"cell_type":"markdown","id":"c2873a75-6da0-4a88-bbee-b63350235de9","metadata":{},"source":["As we can see, unhealthy nutrients such as Saturated Fat, Sugars, Cholesterol, Total Fat, etc., have high positive coefficients. Food items containing unhealthy nutrients will have higher coeficients and will be more likely to be categorized in the 'Less Often' class.\n"]},{"cell_type":"markdown","id":"98dae284-24c6-438c-a576-c6858952f29c","metadata":{},"source":["Next, let's see the coefficents for Class 2, `More Often`:\n"]},{"cell_type":"code","execution_count":null,"id":"e7ad9059-52aa-43d0-a8f1-8ba6111f1e47","metadata":{},"outputs":[],"source":["# Coefficients for Class 2\n","coef_dict = get_feature_coefs(l1_model, 2, feature_cols)\n","visualize_coefs(coef_dict)"]},{"cell_type":"markdown","id":"8a24ef64-fdcf-471f-96e1-ddd01ff4aab5","metadata":{},"source":["Conversely, if a food item has a high amount of calories, total carbohydrates, and total fat, then it is unlikely to be categorized in the 'More Often' class.\n"]},{"cell_type":"markdown","id":"b70efbd6-5db2-4951-b081-6d3e67236329","metadata":{},"source":["## Coding Exercise: Train and evaluate a logistic regression model with elastic-net penality\n"]},{"cell_type":"markdown","id":"0e63179d-fcea-419d-ba3f-5098bb847196","metadata":{},"source":["Now, it's your turn to walk through the end-to-end process of defining, building, evaluating, and interpreting a logistic regression model.\n"]},{"cell_type":"markdown","id":"fbb7806f-e76b-4279-95e3-8b3c507fb96f","metadata":{},"source":["### Define a logistic regression with elastic-net penality\n"]},{"cell_type":"code","execution_count":null,"id":"c4e3f809-b6ab-4dd6-ac93-c2d354047a1a","metadata":{},"outputs":[],"source":["# Type your code here\n","# HINT: sklearn only support saga solver for elastic-net penality\n","# and you need to set another l1_ratio to be within 0 < l1_ratio <1, in order to actually use elastic-net\n"]},{"cell_type":"markdown","id":"129cf758-d022-4cf1-9111-999acd3e70dc","metadata":{},"source":["### Train the model with training data\n"]},{"cell_type":"code","execution_count":null,"id":"6166b8f5-f188-4628-881e-80fc3e64aa30","metadata":{},"outputs":[],"source":["# Type your code here\n"]},{"cell_type":"markdown","id":"026388d5-6715-47cb-a357-735bed484c3e","metadata":{},"source":["### Evaluate the model using accuracy, precision, recall, and F1score\n"]},{"cell_type":"code","execution_count":null,"id":"31f38feb-74b4-4535-b574-0d745884ddef","metadata":{},"outputs":[],"source":["# Type your code here\n"]},{"cell_type":"markdown","id":"3e326daa-3c55-447f-808a-fb8760f74892","metadata":{},"source":["### Plot confusion matrix\n"]},{"cell_type":"code","execution_count":null,"id":"74a5b4ce-55e1-4b83-82a5-0390c25d4ca3","metadata":{},"outputs":[],"source":["# Type your code here\n"]},{"cell_type":"markdown","id":"b36c9d3f-24a4-467d-a8e9-d7358be2319a","metadata":{},"source":["### Interpret the model by analysing its coefficients\n"]},{"cell_type":"code","execution_count":null,"id":"82901226-239b-4d0c-87ac-a5e17e09d552","metadata":{},"outputs":[],"source":["# Type your code here\n"]},{"cell_type":"markdown","id":"ded1c7ac-f99a-4a50-99be-1a78febb3c7b","metadata":{},"source":["<details><summary>Click here for a sample solution</summary>\n","\n","```python\n","# elasticnet penalty to shrink coefficients without removing any features from the model\n","penalty= 'elasticnet'\n","# Our classification problem is multinomial\n","multi_class = 'multinomial'\n","# Use saga for L1 penalty and multinomial classes\n","solver = 'saga'\n","# Max iteration = 1000\n","max_iter = 1000\n","# l1_ratio\n","l1_ratio = 0.1\n","\n","# Define a elastic-net model\n","en_model = LogisticRegression(random_state=rs, penalty=penalty, multi_class=multi_class, solver=solver, max_iter = 1000, l1_ratio=l1_ratio)\n","en_model.fit(X_train, y_train)\n","# Make predictions\n","preds = en_model.predict(X_test)\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","id":"8b2df67b-3d21-4b83-af4e-dedef7e0f285","metadata":{},"source":["## Next steps\n"]},{"cell_type":"markdown","id":"0c24a5af-2734-4197-835d-ff882c021e56","metadata":{},"source":["Great! Now you have learned about and practiced applying a logistic regression model to solve a real-world food classification problem for diabetic patients. You also learned how to evaluate and interpret the trained logistic regression models.\n"]},{"cell_type":"markdown","id":"1fff8222-9e64-4cfc-b854-c5ebb52cde24","metadata":{},"source":["Next, you will be learning other popular classification models with different structures, assumptions, cost functions, and application scenarios.\n"]},{"cell_type":"markdown","id":"e4007e9b-b8cd-4772-bd74-6c656ab6914a","metadata":{},"source":["## Authors\n"]},{"cell_type":"markdown","id":"16d96101-1285-4cc2-8939-c7ff0f711c71","metadata":{},"source":["[Yan Luo](https://www.linkedin.com/in/yan-luo-96288783/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML241ENSkillsNetwork31576874-2021-01-01)\n"]},{"cell_type":"markdown","id":"fa1a1661-512a-4ba8-b639-8a97823a1c30","metadata":{},"source":["### Other Contributors\n"]},{"cell_type":"markdown","id":"f47c8cc4-7e39-430c-b1b6-33d1078c2f4a","metadata":{},"source":["## Change Log\n"]},{"cell_type":"markdown","id":"152591f1-05eb-465d-b869-d1d75db3123f","metadata":{},"source":["| Date (YYYY-MM-DD) | Version | Changed By | Change Description          |\n","| ----------------- | ------- | ---------- | --------------------------- |\n","| 2021-10-25        | 1.0     | Yan        | Created the initial version |\n"]},{"cell_type":"markdown","id":"968cc834-6cd8-4f0c-b63c-b1c9cde20ffb","metadata":{},"source":["Copyright  2021 IBM Corporation. All rights reserved.\n"]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":4}
